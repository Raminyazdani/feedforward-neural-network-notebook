Ramin Yazdani | Feedforward Neural Network Classifier | main | WIP(experiments): Test high learning rate (config_2)

Experimented with a very high learning rate to demonstrate training failure and understand
the consequences of poor hyperparameter choices. This shows what doesn't work and why.

Configuration 2 (config_2):
- Learning rate: 0.5 (50Ã— higher than baseline)
- Hidden dimensions: 64 (same as baseline)
- Epochs: 100
- Batch size: 32

Key observations documented:
- Training instability with oscillating loss
- Poor convergence or divergence
- Model fails to learn effectively
- Erratic decision boundaries that don't properly separate classes

Analysis:
A learning rate that's too high causes:
- Large weight updates that overshoot optimal values
- Loss oscillating or even exploding
- Inability to converge to a good solution
- Unstable gradients leading to poor performance

Why document failure: Understanding what doesn't work is as valuable as knowing what does.
This experiment demonstrates:
1. Hyperparameters matter significantly
2. Learning rate sensitivity in neural networks
3. How to diagnose training problems through loss curves
4. The importance of proper hyperparameter tuning

Verification: Training completed but with expected poor results. Loss curves show
instability, confirming the learning rate is too high for this problem.
