Ramin Yazdani | Feedforward Neural Network Classifier | main | WIP(training): Add metrics tracking (⚠️ optimizer config bug)

Extended the training loop to track and log metrics per epoch, including loss and accuracy.
However, this commit contains a bug: the learning rate is hardcoded instead of using the
configuration parameter.

Key implementations:
- Added accuracy calculation per epoch
- Added training metrics logging and storage
- Added progress printing for monitoring training
- Added optimizer initialization (Adam)

THE BUG:
Instead of using `lr=config['learning_rate']`, the optimizer was initialized with
hardcoded `lr=0.1`:
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # WRONG - hardcoded!
```

Impact: When testing, the loss exploded. The hardcoded 0.1 learning rate is too high for
this problem, causing training instability. This prevents proper training and makes
hyperparameter experiments impossible.

Root cause: Copy-paste error during refactoring - forgot to parameterize the hardcoded value.
This is a realistic mistake that happens in real development.

This bug will be fixed in the next commit after observing training failure.
