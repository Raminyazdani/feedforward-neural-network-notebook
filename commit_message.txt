Ramin Yazdani | Feedforward Neural Network Classifier | main | WIP(training): Fix optimizer to use config parameter (âœ… hotfix)

Immediately corrected the optimizer initialization to properly use the learning rate from
the configuration dictionary. This fixes the training instability caused by the hardcoded
learning rate in step 8.

Key fix:
Changed from:
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # hardcoded
```

To:
```python
optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])  # parameterized
```

How the bug was found: After a quick test run, the loss was exploding and the model wasn't
learning. I checked the optimizer setup and immediately spotted the hardcoded 0.1 value.

Why this matters: This fix enables proper hyperparameter tuning in later experiments. Without
this, we couldn't test different learning rates, which is crucial for understanding training
dynamics and finding optimal configurations.

Verification: Training now works correctly with different learning rates. Model converges
properly with appropriate learning rate values from configuration.

This demonstrates good debugging practice: catch bugs early through testing, investigate
the root cause, and fix immediately before proceeding.
