Ramin Yazdani | Feedforward Neural Network Classifier | main | WIP(training): Implement core training loop structure

Created the training function with the basic training loop structure, loss computation,
and backpropagation. This implements the fundamental training mechanics.

Key implementations:
- Created train() function with epoch loop
- Added CrossEntropyLoss as the loss function (standard for multi-class classification)
- Implemented backpropagation with loss.backward()
- Added optimizer.step() for parameter updates
- Set up basic epoch iteration structure

The training loop follows the standard PyTorch pattern:
1. Forward pass through the network
2. Compute loss using CrossEntropyLoss
3. Backward pass to compute gradients
4. Optimizer step to update weights

CrossEntropyLoss is chosen because it combines LogSoftmax and NLLLoss, making it ideal
for multi-class classification problems.

Verification: Training loop structure is complete and follows PyTorch best practices.
Ready for metrics tracking and optimizer configuration.
