Ramin Yazdani | Feedforward Neural Network Classifier | main | WIP(model): Implement forward pass with ReLU activation

Completed the forward pass logic with ReLU activation function between layers.
This is where the network gains the ability to learn non-linear patterns.

Key implementations:
- Implemented forward() method in FFNN class
- Added ReLU activation after the first fully connected layer
- Ensured proper tensor flow: input → fc1 → ReLU → fc2 → output
- Verified output dimensions match expected class count

The ReLU (Rectified Linear Unit) activation introduces non-linearity into the network.
Without this non-linear activation, the multi-layer network would collapse to a simple
linear transformation, unable to learn complex decision boundaries. ReLU is chosen for
its computational efficiency and effectiveness in deep learning.

Verification: Forward pass produces correct output shape (batch_size, 3) for 3 classes.
Tested with sample input to ensure tensors flow correctly through the network.
